{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7746386e",
   "metadata": {},
   "source": [
    "<h1><center>Predicción de niveles de contaminación por partículas en Madrid mediante técnicas de aprendizaje profundo</center></h1>\n",
    "\n",
    "<div style=\"text-align: center\">Trabajo Fin de Máster</div>\n",
    "<div style=\"text-align: center\">Máster en Inteligencia Artificial Aplicada (UC3M)</div>\n",
    "<div style=\"text-align: center\">Laura Lillo Collado</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c51646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "import contextily as ctx\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "import seaborn as sns\n",
    "from retry_requests import retry\n",
    "from scipy.stats import normaltest, zscore\n",
    "from shapely.geometry import Point\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, Dense, Flatten, MaxPooling1D\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a1a41",
   "metadata": {},
   "source": [
    "## Metodología\n",
    "\n",
    "1. [Comprensión de datos](#1.)\n",
    "\n",
    "    1.1. [Extracción de datos](#1.1.)\n",
    "    <br><br>\n",
    "    1.2. [Integración de datos](#1.2.)\n",
    "    <br><br>\n",
    "    1.3. [Estudio preliminar de datos](#1.3.)\n",
    "    <br><br>\n",
    "2. [Preparación de datos](#2.)\n",
    "<br><br>\n",
    "3. [Modelado y Evaluación](#3.)\n",
    "\n",
    "    3.1. [Modelos Naive](#3.1.)\n",
    "    <br><br>\n",
    "    3.2. [Estación 8](#3.2.)\n",
    "    <br><br>\n",
    "    3.3. [Estación 18](#3.3.)\n",
    "    <br><br>\n",
    "    3.4. [Estación 24](#3.4.)\n",
    "    <br><br>\n",
    "    3.5. [Estación 36](#3.5.)\n",
    "    <br><br>\n",
    "    3.6. [Estación 38](#3.6.)\n",
    "    <br><br>\n",
    "    3.7. [Estación 40](#3.7.)\n",
    "    <br><br>\n",
    "    3.8. [Estación 47](#3.8.)\n",
    "    <br><br>\n",
    "    3.9. [Estación 48](#3.9.)\n",
    "    <br><br>\n",
    "    3.10. [Estación 50](#3.10.)\n",
    "    <br><br>\n",
    "    3.11. [Estación 55](#3.11.)\n",
    "    <br><br>\n",
    "    3.12. [Estación 57](#3.12.)\n",
    "    <br><br>\n",
    "    3.13. [Estación 60](#3.13.)\n",
    "    <br>\n",
    "4. [Análisis de resultados](#4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3427f",
   "metadata": {},
   "source": [
    "### 1. Comprensión de datos <a class=\"anchor\" id=\"1.\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2d8db",
   "metadata": {},
   "source": [
    "#### 1.1. Extracción de datos <a class=\"anchor\" id=\"1.1.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos con información de las estaciones de control de contaminación\n",
    "estaciones_medicion  = pd.read_csv('informacion_estaciones_red_calidad_aire.csv', sep=';')\n",
    "info_estaciones = estaciones_medicion[estaciones_medicion['PM10']=='X'][['ESTACION', 'CODIGO_CORTO', 'LONGITUD', 'LATITUD']]\n",
    "info_estaciones.columns = ['NombreEstacion', 'CodigoEstacion', 'Longitud', 'Latitud']\n",
    "info_estaciones['NombreEstacion'] = info_estaciones['NombreEstacion'].astype(str)\n",
    "info_estaciones['CodigoEstacion'] = info_estaciones['CodigoEstacion'].astype(str)\n",
    "info_estaciones['Longitud'] = info_estaciones['Longitud'].astype(float)\n",
    "info_estaciones['Latitud'] = info_estaciones['Latitud'].astype(float)\n",
    "\n",
    "# Creación de dataframe con datos sobre calidad del aire\n",
    "\"\"\"\n",
    "directorio_zips = 'C:\\\\Users\\\\Laura Lillo\\\\OneDrive\\\\Master\\\\TFM\\\\Datos_CalidadAire'\n",
    "archivo_salida_calidad_aire = 'datos_calidad_aire_pm10.txt'\n",
    "datos_calidad_aire_recopilados = []\n",
    "for archivo_zip in os.listdir(directorio_zips):\n",
    "    if archivo_zip.endswith('.zip'):\n",
    "        anio = int(archivo_zip[4:8])\n",
    "        ruta_zip = os.path.join(directorio_zips, archivo_zip)\n",
    "        with zipfile.ZipFile(ruta_zip, 'r') as zip_ref:\n",
    "            for nombre_archivo in zip_ref.namelist():\n",
    "                if nombre_archivo.endswith('.txt'):\n",
    "                    with zip_ref.open(nombre_archivo) as archivo_txt:\n",
    "                        for linea in archivo_txt:\n",
    "                            datos = linea.decode('utf-8').split('\\n')[0].split('\\r')[0].split(',')                        \n",
    "                            datos_calidad_aire_recopilados.append(datos)\n",
    "\n",
    "columnas_datos_zip = [\"Provincia\", \"Municipio\", \"CodigoEstacion\", \"Magnitud\", \"Tecnica\", \"PeriodoAnalisis\", \"Anyo\", \"Mes\", \"Dia\"] \n",
    "columnas_datos_zip_horas = [f\"H{i:02d}\" for i in range(1, 25)]\n",
    "columnas_datos_zip_v = [f\"V{i:02d}\" for i in range(1, 25)]\n",
    "columnas_datos_zip_intercaladas = [col for pair in zip(columnas_datos_zip_horas, columnas_datos_zip_v) for col in pair]\n",
    "columnas_datos_zip += columnas_datos_zip_intercaladas\n",
    "datos_calidad_aire = pd.DataFrame(datos_calidad_aire_recopilados, columns=columnas_datos_zip)\n",
    "datos_pm10 = datos_calidad_aire[datos_calidad_aire['Magnitud']=='10']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "datos_pm10['Provincia'] = datos_pm10['Provincia'].astype(str)\n",
    "datos_pm10['Municipio'] = datos_pm10['Municipio'].astype(str)\n",
    "datos_pm10['CodigoEstacion'] = datos_pm10['CodigoEstacion'].astype(int).astype(str)\n",
    "datos_pm10['Magnitud'] = datos_pm10['Magnitud'].astype(str)\n",
    "datos_pm10['Tecnica'] = datos_pm10['Tecnica'].astype(str)\n",
    "datos_pm10['PeriodoAnalisis'] = datos_pm10['PeriodoAnalisis'].astype(str)\n",
    "\n",
    "datos_pm10[columnas_datos_zip_horas] = datos_pm10[columnas_datos_zip_horas].astype(float)\n",
    "datos_pm10[columnas_datos_zip_v] = datos_pm10[columnas_datos_zip_v].applymap(lambda x: True if x == 'V' else False)\n",
    "\n",
    "if len(datos_pm10['Provincia'].unique())==1:\n",
    "    datos_pm10.drop('Provincia', axis=1, inplace=True)\n",
    "if len(datos_pm10['Municipio'].unique())==1:\n",
    "    datos_pm10.drop('Municipio', axis=1, inplace=True)\n",
    "if len(datos_pm10['CodigoEstacion'].unique())==1:\n",
    "    datos_pm10.drop('CodigoEstacion', axis=1, inplace=True)\n",
    "if len(datos_pm10['Magnitud'].unique())==1:\n",
    "    datos_pm10.drop('Magnitud', axis=1, inplace=True)\n",
    "if len(datos_pm10['Tecnica'].unique())==1:\n",
    "    datos_pm10.drop('Tecnica', axis=1, inplace=True)\n",
    "if len(datos_pm10['PeriodoAnalisis'].unique())==1:\n",
    "    datos_pm10.drop('PeriodoAnalisis', axis=1, inplace=True)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "pm10datoss = datos_pm10.copy()\n",
    "pm10datoss['CodigoEstacion'] = pm10datoss['CodigoEstacion'].astype(str)\n",
    "pm10datoss['aux'] = pm10datoss['Anyo'].astype(str) + '-' + pm10datoss['Mes'].astype(str) + '-' + pm10datoss['Dia'].astype(str)\n",
    "pm10datoss['Fecha'] = pd.to_datetime(pm10datoss['aux'], format='%Y-%m-%d')\n",
    "pm10datoss.drop(['aux'], axis=1, inplace=True)\n",
    "pm10datoss.index = pm10datoss['Fecha']\n",
    "#pm10datoss.drop(['Fecha'], axis=1, inplace=True)\n",
    "pm10datoss.drop(['Anyo'], axis=1, inplace=True)\n",
    "pm10datoss.drop(['Mes'], axis=1, inplace=True)\n",
    "pm10datoss.drop(['Dia'], axis=1, inplace=True)\n",
    "for i in range(1, 25):\n",
    "    hora_col = f'H{i:02d}'\n",
    "    val_col = f'V{i:02d}'\n",
    "    pm10datoss.loc[~pm10datoss[val_col], hora_col] = float('NaN')\n",
    "    pm10datoss.drop([val_col], axis=1, inplace=True)\n",
    "pm10datoss.to_csv('datosPM10.csv', index=False)\n",
    "\"\"\"\n",
    "# Lectura de datos calidad del aire\n",
    "pm10 = pd.read_csv('datosPM10.csv')\n",
    "pm10['CodigoEstacion'] = pm10['CodigoEstacion'].astype(str)\n",
    "pm10['Fecha'] = pd.to_datetime(pm10['Fecha'], format='%Y-%m-%d')\n",
    "pm10 = pm10.sort_values(by=['Fecha'])\n",
    "pm10.index = pm10['Fecha']\n",
    "pm10.drop(['Fecha'], axis=1, inplace=True)\n",
    "pm10 = pm10[pm10['CodigoEstacion'].isin(info_estaciones['CodigoEstacion'].unique())]\n",
    "\n",
    "\"\"\"\n",
    "todo_meteorologia = pd.DataFrame()\n",
    "for fila in range(len(info_estaciones)):\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "    # Make sure all required weather variables are listed here\n",
    "    # The order of variables in hourly or daily is important to assign them correctly below\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": info_estaciones.iloc[fila]['Latitud'],\n",
    "        \"longitude\": info_estaciones.iloc[fila]['Longitud'],\n",
    "        \"start_date\": \"2010-01-01\",\n",
    "        \"end_date\": \"2023-12-31\",\n",
    "        \"hourly\":  [\"temperature_2m\", \n",
    "                    \"relative_humidity_2m\", \n",
    "                    \"precipitation\", \n",
    "                    \"rain\", \n",
    "                    \"surface_pressure\", \n",
    "                    \"cloud_cover\", \n",
    "                    \"cloud_cover_low\", \n",
    "                    \"cloud_cover_mid\", \n",
    "                    \"cloud_cover_high\", \n",
    "                    \"wind_speed_10m\", \n",
    "                    \"wind_direction_10m\", \n",
    "                    \"is_day\"]\n",
    "    }\n",
    "    #responses = openmeteo.weather_api(url, params=params)\n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "    except Exception as e:\n",
    "        if 'Minutely API request limit exceeded' in str(e):\n",
    "            print('Rate limit exceeded. Waiting for 1 minute...')\n",
    "            time.sleep(60)\n",
    "            # Retry the API request\n",
    "            responses = openmeteo.weather_api(url, params=params)\n",
    "        elif 'Hourly API request limit exceeded' in str(e):\n",
    "            print('Rate limit exceeded. Waiting for 1 hour...')\n",
    "            time.sleep(60*60)\n",
    "            # Retry the API request\n",
    "            responses = openmeteo.weather_api(url, params=params)\n",
    "        else:\n",
    "            raise  # Propagate other errors\n",
    "    # Process first location. Add a for-loop for multiple locations or weather models\n",
    "    response = responses[0]\n",
    "\n",
    "    # Process hourly data. The order of variables needs to be the same as requested.\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "    hourly_surface_pressure = hourly.Variables(4).ValuesAsNumpy()\n",
    "    hourly_cloud_cover = hourly.Variables(5).ValuesAsNumpy()\n",
    "    hourly_cloud_cover_low = hourly.Variables(6).ValuesAsNumpy()\n",
    "    hourly_cloud_cover_mid = hourly.Variables(7).ValuesAsNumpy()\n",
    "    hourly_cloud_cover_high = hourly.Variables(8).ValuesAsNumpy()\n",
    "    hourly_wind_speed_10m = hourly.Variables(9).ValuesAsNumpy()\n",
    "    hourly_wind_direction_10m = hourly.Variables(10).ValuesAsNumpy()\n",
    "    hourly_is_day = hourly.Variables(11).ValuesAsNumpy()\n",
    "    hourly_data = {\"FechaHora\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    )}\n",
    "    hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "    hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "    hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "    hourly_data[\"rain\"] = hourly_rain\n",
    "    hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
    "    hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "    hourly_data[\"cloud_cover_low\"] = hourly_cloud_cover_low\n",
    "    hourly_data[\"cloud_cover_mid\"] = hourly_cloud_cover_mid\n",
    "    hourly_data[\"cloud_cover_high\"] = hourly_cloud_cover_high\n",
    "    hourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "    hourly_data[\"wind_direction_10m\"] = hourly_wind_direction_10m\n",
    "    hourly_data[\"is_day\"] = hourly_is_day\n",
    "    hourly_data['CodigoEstacion'] = info_estaciones.iloc[fila]['CodigoEstacion']\n",
    "    hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "    todo_meteorologia = pd.concat([todo_meteorologia, hourly_dataframe])\n",
    "todo_meteorologia['FechaHora'] = todo_meteorologia['FechaHora'].dt.tz_localize(None)\n",
    "todo_meteorologia.to_csv('meteorologia_v2.csv', index=False)\n",
    "\"\"\"\n",
    "\n",
    "meteorologia = pd.read_csv('meteorologia_v2.csv')\n",
    "meteorologia['CodigoEstacion'] = meteorologia['CodigoEstacion'].astype(str)\n",
    "meteorologia['FechaHora'] = pd.to_datetime(meteorologia['FechaHora'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "with open(\"festivos_locales_historicos.json\", 'r', encoding='utf-8') as archivo:\n",
    "    locales_json = json.load(archivo)\n",
    "with open(\"festivos_regionales_historicos.json\", 'r', encoding='utf-8') as archivo:\n",
    "    regionales_json = json.load(archivo)\n",
    "festivos_locales = pd.DataFrame(locales_json['data'])\n",
    "festivos_regionales = pd.DataFrame(regionales_json['data'])\n",
    "festivos_locales['fecha_festivo'] = pd.to_datetime(festivos_locales['fecha_festivo']).dt.date\n",
    "festivos_regionales['fecha_festivo'] = pd.to_datetime(festivos_regionales['fecha_festivo']).dt.date\n",
    "locales = festivos_locales[festivos_locales['municipio_nombre']=='Madrid']\n",
    "locales = locales[(locales['fecha_festivo'] >= pd.to_datetime('2010-01-01').date()) & (locales['fecha_festivo'] <= pd.to_datetime('2023-12-31').date())] \n",
    "regionales = festivos_regionales[(festivos_regionales['fecha_festivo'] >= pd.to_datetime('2010-01-01').date()) & (festivos_regionales['fecha_festivo'] <= pd.to_datetime('2023-12-31').date())]\n",
    "fechas_festivos = list(locales['fecha_festivo']) + list(regionales['fecha_festivo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea86230",
   "metadata": {},
   "source": [
    "#### 1.2. Integración de datos <a class=\"anchor\" id=\"1.2.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b697118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print('FECHAS FALTANTES')\n",
    "print('================')\n",
    "for e in pm10['CodigoEstacion'].unique():\n",
    "    print(f'Estación {e}')\n",
    "    pm10_est = pm10[pm10['CodigoEstacion']==e]\n",
    "    fecha_minima = pm10_est.index.min().date()\n",
    "    fecha_maxima = pm10_est.index.max().date()\n",
    "    if fecha_maxima - fecha_minima == datetime.timedelta(days=len(set(pm10_est.index.date)) - 1):\n",
    "        print(\"\\tTodos los días están presentes en el rango de fechas.\")\n",
    "    else:\n",
    "        print(\"\\tFaltan días en el rango de fechas.\")\n",
    "print()\n",
    "print('IMPUTACIÓN DE FECHAS FALTANTES')\n",
    "print('==============================')\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "freq = \"D\"\n",
    "all_timestamps = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "for e in pm10['CodigoEstacion'].unique():\n",
    "    print(e)\n",
    "    pm10_estacion = pm10[pm10['CodigoEstacion']==e]\n",
    "    missing_timestamps = set(all_timestamps).difference(pm10_estacion.index)\n",
    "    for timestamp in missing_timestamps:\n",
    "        nueva_fila = pd.DataFrame([[e] + [np.nan]*24], columns=pm10.columns, index=[timestamp])\n",
    "        pm10 = pd.concat([pm10, nueva_fila])\n",
    "print()\n",
    "print('FECHAS FALTANTE TRAS IMPUTACIÓN')\n",
    "print('===============================')\n",
    "for e in pm10['CodigoEstacion'].unique():\n",
    "    print(f'Estación {e}')\n",
    "    pm10_est = pm10[pm10['CodigoEstacion']==e]\n",
    "    fecha_minima = pm10_est.index.min().date()\n",
    "    fecha_maxima = pm10_est.index.max().date()\n",
    "\n",
    "    # Verificar si todos los días están presentes\n",
    "    if fecha_maxima - fecha_minima == datetime.timedelta(days=len(set(pm10_est.index.date)) - 1):\n",
    "        print(\"\\tTodos los días están presentes en el rango de fechas.\")\n",
    "    else:\n",
    "        print(\"\\tFaltan días en el rango de fechas.\")\n",
    "        \n",
    "pm10 = pm10.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bbf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def serie_temporal_estacion(estacion):\n",
    "    columnas_datos_zip_horas = pm10.drop(['CodigoEstacion'], axis=1, inplace=False).columns\n",
    "    prueba_e = pm10[pm10['CodigoEstacion']==estacion]\n",
    "    estaciones = []\n",
    "    fechashoras = []\n",
    "    nivelespm10 = []\n",
    "    for i in range(len(prueba_e)):\n",
    "        fila = prueba_e.iloc[i]\n",
    "        estacion = fila['CodigoEstacion']\n",
    "        fecha = fila.name.date()\n",
    "        for ch in columnas_datos_zip_horas:\n",
    "            h = ch.split('H')[1]\n",
    "            h2 = str(int(h)-1).zfill(2)\n",
    "            hora = h2+':00:00'\n",
    "            fecha_hora = str(fecha)+' '+hora\n",
    "            fecha_hora = datetime.strptime(fecha_hora, '%Y-%m-%d %H:%M:%S')\n",
    "            dato = fila[ch]\n",
    "            estaciones.append(estacion)\n",
    "            fechashoras.append(fecha_hora)\n",
    "            nivelespm10.append(dato)\n",
    "\n",
    "    dic_aux = {'CodigoEstacion': estaciones,\n",
    "               'FechaHora': fechashoras,\n",
    "               'NivelesPM10': nivelespm10}\n",
    "\n",
    "    st = pd.DataFrame(dic_aux)\n",
    "    st = st.sort_values(by=['CodigoEstacion', 'FechaHora'])\n",
    "    return st\n",
    "\n",
    "pm10_st = pd.DataFrame()\n",
    "for e in pm10['CodigoEstacion'].unique():\n",
    "    st_e = serie_temporal_estacion(e)\n",
    "    pm10_st = pd.concat([pm10_st, st_e])\n",
    "    \n",
    "df_merged = pd.merge(pm10_st, meteorologia, on=['FechaHora', 'CodigoEstacion'])\n",
    "df_merged['Festivo'] = df_merged['FechaHora'].apply(lambda x: 1 if x.date() in fechas_festivos else 0)\n",
    "inicio_confinamiento = pd.to_datetime('2020-03-14 00:00:00')\n",
    "fin_confinamiento = pd.to_datetime('2020-05-02 23:59:59')\n",
    "df_merged['ConfinamientoPandemia'] = ((df_merged['FechaHora'] >= inicio_confinamiento) & (df_merged['FechaHora'] <= fin_confinamiento)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042b62b",
   "metadata": {},
   "source": [
    "#### 1.3.Estudio preliminar de los datos<a class=\"anchor\" id=\"1.3.\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38976513",
   "metadata": {},
   "source": [
    "Datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a167e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos faltantes de Niveles de PM10 por estación\n",
    "df = pm10.copy()\n",
    "fig, axs = plt.subplots(len(df['CodigoEstacion'].unique()), 1, figsize=(12, 4*len(df['CodigoEstacion'].unique())))\n",
    "for i, estacion in enumerate(df['CodigoEstacion'].unique()):\n",
    "    df_estacion = df[df['CodigoEstacion'] == estacion]\n",
    "    datos_faltantes = df_estacion.isnull().any(axis=1)\n",
    "    porcentaje_faltantes_e = df_estacion.isnull().sum().sum() / (df_estacion.shape[0] * pm10.shape[1]) * 100\n",
    "    print(f\"Porcentaje de observaciones faltantes estacion {estacion}:\", porcentaje_faltantes_e)\n",
    "    axs[i].plot(df_estacion.index, datos_faltantes.astype(int), color='lightgrey', linewidth=1)\n",
    "    axs[i].fill_between(df_estacion.index, datos_faltantes.astype(int), color='red', alpha=0.3)\n",
    "    axs[i].set_title(f'Datos faltantes - Estación {estacion}')\n",
    "    axs[i].set_ylabel('Datos faltantes')\n",
    "    axs[i].set_yticks([0, 1])\n",
    "    axs[i].set_yticklabels(['No faltante', 'Faltante'])\n",
    "    axs[i].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4da7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos estación 56\n",
    "estacion_a_eliminar = '56'\n",
    "df_merged = df_merged.drop(df_merged[df_merged['CodigoEstacion'] == estacion_a_eliminar].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4edac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['CodigoEstacion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733fd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc47fdb",
   "metadata": {},
   "source": [
    "Imputación datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31067228",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_imputado = df_merged.copy()\n",
    "for e in df_merged['CodigoEstacion'].unique():\n",
    "    pm10_e = df_merged[df_merged['CodigoEstacion'] == e].copy()\n",
    "    scaler = StandardScaler()\n",
    "    x = pm10_e.select_dtypes(include='number')\n",
    "    num_cols = list(x.columns)\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    imputer = KNNImputer()\n",
    "    imputed_data = imputer.fit_transform(x_scaled)\n",
    "    imputed_data_original_scale = scaler.inverse_transform(imputed_data)\n",
    "    pm10_imputado.loc[pm10_imputado['CodigoEstacion'] == e, num_cols] = imputed_data_original_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afcaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_imputado.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_imputado.columns = ['CodigoEstacion', 'FechaHora', 'NivelesPM10', 'Temperatura', 'HumedadRelativa', 'Precipitacion',\n",
    "                        'LLuvia', 'PresionSuperficie', 'NubosidadTotal', 'NubosidadBaja', 'NubosidadMedia', 'NubosidadAlta', \n",
    "                        'VelocidadViento', 'DireccionViento', 'DiaNoche', 'Festivo', 'ConfinamientoPandemia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f4428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm10_imputado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc46bbf",
   "metadata": {},
   "source": [
    "Datos atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ebbd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pm10_imputado.copy()\n",
    "\n",
    "fig, axs = plt.subplots(len(df['CodigoEstacion'].unique()), 3, figsize=(18, 4*len(df['CodigoEstacion'].unique())))\n",
    "\n",
    "for i, estacion in enumerate(df['CodigoEstacion'].unique()):\n",
    "    df_estacion = df[df['CodigoEstacion'] == estacion]\n",
    "\n",
    "    # Histograma\n",
    "    axs[i, 0].hist(df_estacion['NivelesPM10'], bins=20, color='skyblue', edgecolor='black')\n",
    "    axs[i, 0].set_title(f'Histograma de PM10 - Estación {estacion}')\n",
    "    axs[i, 0].set_xlabel('Niveles de PM10')\n",
    "    axs[i, 0].set_ylabel('Frecuencia')\n",
    "    axs[i, 0].grid(True)\n",
    "\n",
    "    # Gráfico de evolución\n",
    "    axs[i, 1].plot(df_estacion['FechaHora'], df_estacion['NivelesPM10'], color='salmon')\n",
    "    axs[i, 1].set_title(f'Evolución de PM10 - Estación {estacion}')\n",
    "    axs[i, 1].set_xlabel('Fecha y Hora')\n",
    "    axs[i, 1].set_ylabel('Niveles de PM10')\n",
    "    axs[i, 1].grid(True)\n",
    "\n",
    "    # Calculando estadísticas\n",
    "    min_val = np.min(df_estacion['NivelesPM10'])\n",
    "    mean_val = np.mean(df_estacion['NivelesPM10'])\n",
    "    median_val = np.median(df_estacion['NivelesPM10'])\n",
    "    max_val = np.max(df_estacion['NivelesPM10'])\n",
    "    std_val = np.std(df_estacion['NivelesPM10'])\n",
    "\n",
    "    # Agregando cuadro de texto con estadísticas\n",
    "    textstr = '\\n'.join((\n",
    "        f'Mínimo: {min_val}',\n",
    "        f'Medio: {mean_val}',\n",
    "        f'Mediano: {median_val}',\n",
    "        f'Máximo: {max_val}',\n",
    "        f'Desviación Típica: {std_val}'))\n",
    "    axs[i, 2].text(0.1, 0.5, textstr, fontsize=10, verticalalignment='center')\n",
    "    axs[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e90e43",
   "metadata": {},
   "source": [
    "### 2. Preparación de datos <a class=\"anchor\" id=\"2.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_scaled = pm10_imputado.copy()\n",
    "scaler = StandardScaler()\n",
    "columns = pm10_scaled.drop(['NivelesPM10','CodigoEstacion', 'FechaHora', 'DiaNoche', 'Festivo', 'ConfinamientoPandemia'], axis=1, inplace=False).columns\n",
    "pm10_scaled[columns] = scaler.fit_transform(pm10_scaled[columns])\n",
    "pm10_scaled.index = pm10_scaled['FechaHora']\n",
    "pm10_scaled.drop(['FechaHora'], axis=1, inplace=True)\n",
    "pm10_scaled = pm10_scaled.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_scaled.to_csv('datosPM10escalados.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02b404",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='8']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Escuelas Aguirre (8))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='18']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Farolillo (18))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='24']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Casa de Campo (24))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='36']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Moratalaz (36))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='38']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Cuatro Caminos (38))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='40']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Vallecas (40))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='47']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Méndez Álvaro (47))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='48']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Castellana (48))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='50']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Plaza Castilla (50))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='55']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Urb. Embajada (55))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='57']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Sanchinarro (57))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "p = pm10_scaled[pm10_scaled['CodigoEstacion']=='60']\n",
    "plt.plot(p.index, p['NivelesPM10'], color='#104861')\n",
    "plt.title('Contaminación por partículas PM10 (estación de control Tres Olivos (60))')\n",
    "plt.ylabel('Niveles PM10')\n",
    "plt.xlabel('Hora')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea2eec",
   "metadata": {},
   "source": [
    "### 3. Modelado y Evaluación <a class=\"anchor\" id=\"3.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y_datasets(dataset, look_back=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-look_back-48):\n",
    "        X.append(dataset.iloc[i:(i+48), 0:])\n",
    "        y.append(dataset.iloc[i + look_back:i + look_back + 48, 0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "e8 = '8'\n",
    "df_e8 = pm10_scaled[pm10_scaled['CodigoEstacion']==e8]\n",
    "df_e8 = df_e8.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e8 = pd.DataFrame()\n",
    "\n",
    "X_e8, y_e8 = create_X_y_datasets(df_e8, look_back=1)\n",
    "\n",
    "indices_train_e8 = df_e8[(df_e8.index.year >= 2010) & (df_e8.index.year <= 2021)].index\n",
    "indices_val_e8 = df_e8[df_e8.index.year == 2022].index\n",
    "indices_test_e8 = df_e8[df_e8.index.year == 2023].index\n",
    "\n",
    "idx_train_e8 = [i for i, date in enumerate(df_e8.index[:-49]) if date in indices_train_e8]\n",
    "idx_val_e8 = [i for i, date in enumerate(df_e8.index[:-49]) if date in indices_val_e8]\n",
    "idx_test_e8 = [i for i, date in enumerate(df_e8.index[:-49]) if date in indices_test_e8]\n",
    "\n",
    "X_train_e8, y_train_e8 = X_e8[idx_train_e8], y_e8[idx_train_e8]\n",
    "X_val_e8, y_val_e8 = X_e8[idx_val_e8], y_e8[idx_val_e8]\n",
    "X_test_e8, y_test_e8 = X_e8[idx_test_e8], y_e8[idx_test_e8]\n",
    "\n",
    "e55 = '55'\n",
    "df_e55 = pm10_scaled[pm10_scaled['CodigoEstacion']==e55]\n",
    "df_e55 = df_e55.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e55 = pd.DataFrame()\n",
    "\n",
    "X_e55, y_e55 = create_X_y_datasets(df_e55, look_back=1)\n",
    "\n",
    "indices_train_e55 = df_e55[(df_e55.index.year >= 2010) & (df_e55.index.year <= 2021)].index\n",
    "indices_val_e55 = df_e55[df_e55.index.year == 2022].index\n",
    "indices_test_e55 = df_e55[df_e55.index.year == 2023].index\n",
    "\n",
    "idx_train_e55 = [i for i, date in enumerate(df_e55.index[:-49]) if date in indices_train_e55]\n",
    "idx_val_e55 = [i for i, date in enumerate(df_e55.index[:-49]) if date in indices_val_e55]\n",
    "idx_test_e55 = [i for i, date in enumerate(df_e55.index[:-49]) if date in indices_test_e55]\n",
    "\n",
    "X_train_e55, y_train_e55 = X_e55[idx_train_e55], y_e55[idx_train_e55]\n",
    "X_val_e55, y_val_e55 = X_e55[idx_val_e55], y_e55[idx_val_e55]\n",
    "X_test_e55, y_test_e55 = X_e55[idx_test_e55], y_e55[idx_test_e55]\n",
    "\n",
    "e36 = '36'\n",
    "df_e36 = pm10_scaled[pm10_scaled['CodigoEstacion']==e36]\n",
    "df_e36 = df_e36.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e36 = pd.DataFrame()\n",
    "\n",
    "X_e36, y_e36 = create_X_y_datasets(df_e36, look_back=1)\n",
    "\n",
    "indices_train_e36 = df_e36[(df_e36.index.year >= 2010) & (df_e36.index.year <= 2021)].index\n",
    "indices_val_e36 = df_e36[df_e36.index.year == 2022].index\n",
    "indices_test_e36 = df_e36[df_e36.index.year == 2023].index\n",
    "\n",
    "idx_train_e36 = [i for i, date in enumerate(df_e36.index[:-49]) if date in indices_train_e36]\n",
    "idx_val_e36 = [i for i, date in enumerate(df_e36.index[:-49]) if date in indices_val_e36]\n",
    "idx_test_e36 = [i for i, date in enumerate(df_e36.index[:-49]) if date in indices_test_e36]\n",
    "\n",
    "X_train_e36, y_train_e36 = X_e36[idx_train_e36], y_e36[idx_train_e36]\n",
    "X_val_e36, y_val_e36 = X_e36[idx_val_e36], y_e36[idx_val_e36]\n",
    "X_test_e36, y_test_e36 = X_e36[idx_test_e36], y_e36[idx_test_e36]\n",
    "\n",
    "e38 = '38'\n",
    "df_e38 = pm10_scaled[pm10_scaled['CodigoEstacion']==e38]\n",
    "df_e38 = df_e38.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e38 = pd.DataFrame()\n",
    "\n",
    "X_e38, y_e38 = create_X_y_datasets(df_e38, look_back=1)\n",
    "\n",
    "indices_train_e38 = df_e38[(df_e38.index.year >= 2010) & (df_e38.index.year <= 2021)].index\n",
    "indices_val_e38 = df_e38[df_e38.index.year == 2022].index\n",
    "indices_test_e38 = df_e38[df_e38.index.year == 2023].index\n",
    "\n",
    "idx_train_e38 = [i for i, date in enumerate(df_e38.index[:-49]) if date in indices_train_e38]\n",
    "idx_val_e38 = [i for i, date in enumerate(df_e38.index[:-49]) if date in indices_val_e38]\n",
    "idx_test_e38 = [i for i, date in enumerate(df_e38.index[:-49]) if date in indices_test_e38]\n",
    "\n",
    "X_train_e38, y_train_e38 = X_e38[idx_train_e38], y_e38[idx_train_e38]\n",
    "X_val_e38, y_val_e38 = X_e38[idx_val_e38], y_e38[idx_val_e38]\n",
    "X_test_e38, y_test_e38 = X_e38[idx_test_e38], y_e38[idx_test_e38]\n",
    "\n",
    "e57 = '57'\n",
    "df_e57 = pm10_scaled[pm10_scaled['CodigoEstacion']==e57]\n",
    "df_e57 = df_e57.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e57 = pd.DataFrame()\n",
    "\n",
    "X_e57, y_e57 = create_X_y_datasets(df_e57, look_back=1)\n",
    "\n",
    "indices_train_e57 = df_e57[(df_e57.index.year >= 2010) & (df_e57.index.year <= 2021)].index\n",
    "indices_val_e57 = df_e57[df_e57.index.year == 2022].index\n",
    "indices_test_e57 = df_e57[df_e57.index.year == 2023].index\n",
    "\n",
    "idx_train_e57 = [i for i, date in enumerate(df_e57.index[:-49]) if date in indices_train_e57]\n",
    "idx_val_e57 = [i for i, date in enumerate(df_e57.index[:-49]) if date in indices_val_e57]\n",
    "idx_test_e57 = [i for i, date in enumerate(df_e57.index[:-49]) if date in indices_test_e57]\n",
    "\n",
    "X_train_e57, y_train_e57 = X_e57[idx_train_e57], y_e57[idx_train_e57]\n",
    "X_val_e57, y_val_e57 = X_e57[idx_val_e57], y_e57[idx_val_e57]\n",
    "X_test_e57, y_test_e57 = X_e57[idx_test_e57], y_e57[idx_test_e57]\n",
    "\n",
    "e60 = '60'\n",
    "df_e60 = pm10_scaled[pm10_scaled['CodigoEstacion']==e60]\n",
    "df_e60 = df_e60.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e60 = pd.DataFrame()\n",
    "\n",
    "X_e60, y_e60 = create_X_y_datasets(df_e60, look_back=1)\n",
    "\n",
    "indices_train_e60 = df_e60[(df_e60.index.year >= 2010) & (df_e60.index.year <= 2021)].index\n",
    "indices_val_e60 = df_e60[df_e60.index.year == 2022].index\n",
    "indices_test_e60 = df_e60[df_e60.index.year == 2023].index\n",
    "\n",
    "idx_train_e60 = [i for i, date in enumerate(df_e60.index[:-49]) if date in indices_train_e60]\n",
    "idx_val_e60 = [i for i, date in enumerate(df_e60.index[:-49]) if date in indices_val_e60]\n",
    "idx_test_e60 = [i for i, date in enumerate(df_e60.index[:-49]) if date in indices_test_e60]\n",
    "\n",
    "X_train_e60, y_train_e60 = X_e60[idx_train_e60], y_e60[idx_train_e60]\n",
    "X_val_e60, y_val_e60 = X_e60[idx_val_e60], y_e60[idx_val_e60]\n",
    "X_test_e60, y_test_e60 = X_e60[idx_test_e60], y_e60[idx_test_e60]\n",
    "\n",
    "e48 = '48'\n",
    "df_e48 = pm10_scaled[pm10_scaled['CodigoEstacion']==e48]\n",
    "df_e48 = df_e48.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e48 = pd.DataFrame()\n",
    "\n",
    "X_e48, y_e48 = create_X_y_datasets(df_e48, look_back=1)\n",
    "\n",
    "indices_train_e48 = df_e48[(df_e48.index.year >= 2010) & (df_e48.index.year <= 2021)].index\n",
    "indices_val_e48 = df_e48[df_e48.index.year == 2022].index\n",
    "indices_test_e48 = df_e48[df_e48.index.year == 2023].index\n",
    "\n",
    "idx_train_e48 = [i for i, date in enumerate(df_e48.index[:-49]) if date in indices_train_e48]\n",
    "idx_val_e48 = [i for i, date in enumerate(df_e48.index[:-49]) if date in indices_val_e48]\n",
    "idx_test_e48 = [i for i, date in enumerate(df_e48.index[:-49]) if date in indices_test_e48]\n",
    "\n",
    "X_train_e48, y_train_e48 = X_e48[idx_train_e48], y_e48[idx_train_e48]\n",
    "X_val_e48, y_val_e48 = X_e48[idx_val_e48], y_e48[idx_val_e48]\n",
    "X_test_e48, y_test_e48 = X_e48[idx_test_e48], y_e48[idx_test_e48]\n",
    "\n",
    "e40 = '40'\n",
    "df_e40 = pm10_scaled[pm10_scaled['CodigoEstacion']==e40]\n",
    "df_e40 = df_e40.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e40 = pd.DataFrame()\n",
    "\n",
    "X_e40, y_e40 = create_X_y_datasets(df_e40, look_back=1)\n",
    "\n",
    "indices_train_e40 = df_e40[(df_e40.index.year >= 2010) & (df_e40.index.year <= 2021)].index\n",
    "indices_val_e40 = df_e40[df_e40.index.year == 2022].index\n",
    "indices_test_e40 = df_e40[df_e40.index.year == 2023].index\n",
    "\n",
    "idx_train_e40 = [i for i, date in enumerate(df_e40.index[:-49]) if date in indices_train_e40]\n",
    "idx_val_e40 = [i for i, date in enumerate(df_e40.index[:-49]) if date in indices_val_e40]\n",
    "idx_test_e40 = [i for i, date in enumerate(df_e40.index[:-49]) if date in indices_test_e40]\n",
    "\n",
    "X_train_e40, y_train_e40 = X_e40[idx_train_e40], y_e40[idx_train_e40]\n",
    "X_val_e40, y_val_e40 = X_e40[idx_val_e40], y_e40[idx_val_e40]\n",
    "X_test_e40, y_test_e40 = X_e40[idx_test_e40], y_e40[idx_test_e40]\n",
    "\n",
    "e50 = '50'\n",
    "df_e50 = pm10_scaled[pm10_scaled['CodigoEstacion']==e50]\n",
    "df_e50 = df_e50.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e50 = pd.DataFrame()\n",
    "\n",
    "X_e50, y_e50 = create_X_y_datasets(df_e50, look_back=1)\n",
    "\n",
    "indices_train_e50 = df_e50[(df_e50.index.year >= 2010) & (df_e50.index.year <= 2021)].index\n",
    "indices_val_e50 = df_e50[df_e50.index.year == 2022].index\n",
    "indices_test_e50 = df_e50[df_e50.index.year == 2023].index\n",
    "\n",
    "idx_train_e50 = [i for i, date in enumerate(df_e50.index[:-49]) if date in indices_train_e50]\n",
    "idx_val_e50 = [i for i, date in enumerate(df_e50.index[:-49]) if date in indices_val_e50]\n",
    "idx_test_e50 = [i for i, date in enumerate(df_e50.index[:-49]) if date in indices_test_e50]\n",
    "\n",
    "X_train_e50, y_train_e50 = X_e50[idx_train_e50], y_e50[idx_train_e50]\n",
    "X_val_e50, y_val_e50 = X_e50[idx_val_e50], y_e50[idx_val_e50]\n",
    "X_test_e50, y_test_e50 = X_e50[idx_test_e50], y_e50[idx_test_e50]\n",
    "\n",
    "e18 = '18'\n",
    "df_e18 = pm10_scaled[pm10_scaled['CodigoEstacion']==e18]\n",
    "df_e18 = df_e18.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e18 = pd.DataFrame()\n",
    "\n",
    "X_e18, y_e18 = create_X_y_datasets(df_e18, look_back=1)\n",
    "\n",
    "indices_train_e18 = df_e18[(df_e18.index.year >= 2010) & (df_e18.index.year <= 2021)].index\n",
    "indices_val_e18 = df_e18[df_e18.index.year == 2022].index\n",
    "indices_test_e18 = df_e18[df_e18.index.year == 2023].index\n",
    "\n",
    "idx_train_e18 = [i for i, date in enumerate(df_e18.index[:-49]) if date in indices_train_e18]\n",
    "idx_val_e18 = [i for i, date in enumerate(df_e18.index[:-49]) if date in indices_val_e18]\n",
    "idx_test_e18 = [i for i, date in enumerate(df_e18.index[:-49]) if date in indices_test_e18]\n",
    "\n",
    "X_train_e18, y_train_e18 = X_e18[idx_train_e18], y_e18[idx_train_e18]\n",
    "X_val_e18, y_val_e18 = X_e18[idx_val_e18], y_e18[idx_val_e18]\n",
    "X_test_e18, y_test_e18 = X_e18[idx_test_e18], y_e18[idx_test_e18]\n",
    "\n",
    "e47 = '47'\n",
    "df_e47 = pm10_scaled[pm10_scaled['CodigoEstacion']==e47]\n",
    "df_e47 = df_e47.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e47 = pd.DataFrame()\n",
    "\n",
    "X_e47, y_e47 = create_X_y_datasets(df_e47, look_back=1)\n",
    "\n",
    "indices_train_e47 = df_e47[(df_e47.index.year >= 2010) & (df_e47.index.year <= 2021)].index\n",
    "indices_val_e47 = df_e47[df_e47.index.year == 2022].index\n",
    "indices_test_e47 = df_e47[df_e47.index.year == 2023].index\n",
    "\n",
    "idx_train_e47 = [i for i, date in enumerate(df_e47.index[:-49]) if date in indices_train_e47]\n",
    "idx_val_e47 = [i for i, date in enumerate(df_e47.index[:-49]) if date in indices_val_e47]\n",
    "idx_test_e47 = [i for i, date in enumerate(df_e47.index[:-49]) if date in indices_test_e47]\n",
    "\n",
    "X_train_e47, y_train_e47 = X_e47[idx_train_e47], y_e47[idx_train_e47]\n",
    "X_val_e47, y_val_e47 = X_e47[idx_val_e47], y_e47[idx_val_e47]\n",
    "X_test_e47, y_test_e47 = X_e47[idx_test_e47], y_e47[idx_test_e47]\n",
    "\n",
    "e24 = '24'\n",
    "df_e24 = pm10_scaled[pm10_scaled['CodigoEstacion']==e24]\n",
    "df_e24 = df_e24.drop(['CodigoEstacion'], axis=1, inplace=False)\n",
    "\n",
    "evaluacion_e24 = pd.DataFrame()\n",
    "\n",
    "X_e24, y_e24 = create_X_y_datasets(df_e24, look_back=1)\n",
    "\n",
    "indices_train_e24 = df_e24[(df_e24.index.year >= 2010) & (df_e24.index.year <= 2021)].index\n",
    "indices_val_e24 = df_e24[df_e24.index.year == 2022].index\n",
    "indices_test_e24 = df_e24[df_e24.index.year == 2023].index\n",
    "\n",
    "idx_train_e24 = [i for i, date in enumerate(df_e24.index[:-49]) if date in indices_train_e24]\n",
    "idx_val_e24 = [i for i, date in enumerate(df_e24.index[:-49]) if date in indices_val_e24]\n",
    "idx_test_e24 = [i for i, date in enumerate(df_e24.index[:-49]) if date in indices_test_e24]\n",
    "\n",
    "X_train_e24, y_train_e24 = X_e24[idx_train_e24], y_e24[idx_train_e24]\n",
    "X_val_e24, y_val_e24 = X_e24[idx_val_e24], y_e24[idx_val_e24]\n",
    "X_test_e24, y_test_e24 = X_e24[idx_test_e24], y_e24[idx_test_e24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85027321",
   "metadata": {},
   "source": [
    "#### 3.1. Modelos Naive <a class=\"anchor\" id=\"3.1.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = ['e_8', 'e_18', 'e_24', \n",
    "            'e_36', 'e_38', 'e_40', \n",
    "            'e_47', 'e_48', 'e_50', \n",
    "            'e_55', 'e_57', 'e_60']\n",
    "\n",
    "dfs = [df_e8, df_e18, df_e24, \n",
    "       df_e36, df_e38, df_e40, \n",
    "       df_e47, df_e48, df_e50, \n",
    "       df_e55, df_e57, df_e60]\n",
    "\n",
    "idx_trains = [idx_train_e8, idx_train_e18, idx_train_e24, \n",
    "              idx_train_e36, idx_train_e38, idx_train_e40, \n",
    "              idx_train_e47, idx_train_e48, idx_train_e50, \n",
    "              idx_train_e55, idx_train_e57, idx_train_e60]\n",
    "\n",
    "idx_vals = [idx_val_e8, idx_val_e18, idx_val_e24, \n",
    "            idx_val_e36, idx_val_e38, idx_val_e40, \n",
    "            idx_val_e47, idx_val_e48, idx_val_e50, \n",
    "            idx_val_e55, idx_val_e57, idx_val_e60]\n",
    "\n",
    "xs_trains = [X_train_e8, X_train_e18, X_train_e24,\n",
    "             X_train_e36, X_train_e38, X_train_e40,\n",
    "             X_train_e47, X_train_e48, X_train_e50,\n",
    "             X_train_e55, X_train_e57, X_train_e60]\n",
    "\n",
    "ys_trains = [y_train_e8, y_train_e18, y_train_e24,\n",
    "             y_train_e36, y_train_e38, y_train_e40,\n",
    "             y_train_e47, y_train_e48, y_train_e50,\n",
    "             y_train_e55, y_train_e57, y_train_e60]\n",
    "\n",
    "xs_vals = [X_val_e8, X_val_e18, X_val_e24,\n",
    "             X_val_e36, X_val_e38, X_val_e40,\n",
    "             X_val_e47, X_val_e48, X_val_e50,\n",
    "             X_val_e55, X_val_e57, X_val_e60]\n",
    "\n",
    "ys_vals = [y_val_e8, y_val_e18, y_val_e24,\n",
    "             y_val_e36, y_val_e38, y_val_e40,\n",
    "             y_val_e47, y_val_e48, y_val_e50,\n",
    "             y_val_e55, y_val_e57, y_val_e60]\n",
    "\n",
    "xs_tests = [X_test_e8, X_test_e18, X_test_e24,\n",
    "             X_test_e36, X_test_e38, X_test_e40,\n",
    "             X_test_e47, X_test_e48, X_test_e50,\n",
    "             X_test_e55, X_test_e57, X_test_e60]\n",
    "\n",
    "ys_tests = [y_test_e8, y_test_e18, y_test_e24,\n",
    "             y_test_e36, y_test_e38, y_test_e40,\n",
    "             y_test_e47, y_test_e48, y_test_e50,\n",
    "             y_test_e55, y_test_e57, y_test_e60]\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2ss = []\n",
    "rmsess = []\n",
    "preds_lr = {}\n",
    "for i, s in enumerate(stations):\n",
    "    #x_trainval = np.concatenate((xs_trains[i], xs_vals[i]))\n",
    "    #x_trainval = x_trainval.reshape(x_trainval.shape[0], -1)\n",
    "    auxs_st = dfs[i]['NivelesPM10'].iloc[len(idx_trains[i])+len(idx_vals[i])-8:].values\n",
    "    pred_lr = np.empty((ys_tests[i].shape[0], 48))\n",
    "    for j in range(len(ys_tests[i])):\n",
    "        pred_lr[j] = auxs_st[j:j+48]\n",
    "    preds_lr[s] = pred_lr\n",
    "    # Evaluar el modelo\n",
    "    rmse_2 = np.sqrt(np.mean((preds_lr[s] - ys_tests[i]) ** 2))\n",
    "    r2 = r2_score(ys_tests[i], preds_lr[s])\n",
    "    r2ss.append(r2)\n",
    "    rmsess.append(rmse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6da852",
   "metadata": {},
   "source": [
    "#### 3.2. Estación 8 <a class=\"anchor\" id=\"3.2.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ba631",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e8 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e8 = {}\n",
    "resLSTM_u_e8, resLSTM_pred_e8, resLSTM_real_e8, resLSTM_modelos_e8 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e8.shape[1], X_train_e8.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e8, y_train_e8, validation_data=(X_val_e8, y_val_e8), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e8)\n",
    "    \n",
    "    resLSTM_u_e8.append(u)\n",
    "    resLSTM_pred_e8.append(pred_lstm_u)\n",
    "    resLSTM_real_e8.append(y_test_e8)\n",
    "    resLSTM_modelos_e8.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e8['UNITS'] = resLSTM_u_e8\n",
    "resultados_LSTM_e8['PREDICCIONES'] = resLSTM_pred_e8\n",
    "resultados_LSTM_e8['REALES'] = resLSTM_real_e8\n",
    "resultados_LSTM_e8['RED'] = ['LSTM']*len(resLSTM_u_e8)\n",
    "resultados_LSTM_e8['MODELO'] = resLSTM_modelos_e8\n",
    "\n",
    "df_resultados_LSTM_e8 = pd.DataFrame(resultados_LSTM_e8)\n",
    "evaluacion_e8 = pd.concat([evaluacion_e8, df_resultados_LSTM_e8])\n",
    "\n",
    "resultados_GRU_e8 = {}\n",
    "resGRU_u_e8, resGRU_pred_e8, resGRU_real_e8, resGRU_modelos_e8 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e8.shape[1], X_train_e8.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e8, y_train_e8, validation_data=(X_val_e8, y_val_e8), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e8)\n",
    "    \n",
    "    resGRU_u_e8.append(u)\n",
    "    resGRU_pred_e8.append(pred_GRU_u)\n",
    "    resGRU_real_e8.append(y_test_e8)\n",
    "    resGRU_modelos_e8.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e8['UNITS'] = resGRU_u_e8\n",
    "resultados_GRU_e8['PREDICCIONES'] = resGRU_pred_e8\n",
    "resultados_GRU_e8['REALES'] = resGRU_real_e8\n",
    "resultados_GRU_e8['RED'] = ['GRU']*len(resGRU_u_e8)\n",
    "resultados_GRU_e8['MODELO'] = resGRU_modelos_e8\n",
    "\n",
    "df_resultados_GRU_e8 = pd.DataFrame(resultados_GRU_e8)\n",
    "evaluacion_e8 = pd.concat([evaluacion_e8, df_resultados_GRU_e8])\n",
    "evaluacion_e8 = evaluacion_e8.reset_index(drop=True)\n",
    "evaluacion_e8.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac272c4",
   "metadata": {},
   "source": [
    "#### 3.3. Estación 18 <a class=\"anchor\" id=\"3.3.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e18 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e18 = {}\n",
    "resLSTM_u_e18, resLSTM_pred_e18, resLSTM_real_e18, resLSTM_modelos_e18 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e18.shape[1], X_train_e18.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e18, y_train_e18, validation_data=(X_val_e18, y_val_e18), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e18)\n",
    "    \n",
    "    resLSTM_u_e18.append(u)\n",
    "    resLSTM_pred_e18.append(pred_lstm_u)\n",
    "    resLSTM_real_e18.append(y_test_e18)\n",
    "    resLSTM_modelos_e18.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e18['UNITS'] = resLSTM_u_e18\n",
    "resultados_LSTM_e18['PREDICCIONES'] = resLSTM_pred_e18\n",
    "resultados_LSTM_e18['REALES'] = resLSTM_real_e18\n",
    "resultados_LSTM_e18['RED'] = ['LSTM']*len(resLSTM_u_e18)\n",
    "resultados_LSTM_e18['MODELO'] = resLSTM_modelos_e18\n",
    "\n",
    "df_resultados_LSTM_e18 = pd.DataFrame(resultados_LSTM_e18)\n",
    "evaluacion_e18 = pd.concat([evaluacion_e18, df_resultados_LSTM_e18])\n",
    "\n",
    "resultados_GRU_e18 = {}\n",
    "resGRU_u_e18, resGRU_pred_e18, resGRU_real_e18, resGRU_modelos_e18 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e18.shape[1], X_train_e18.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e18, y_train_e18, validation_data=(X_val_e18, y_val_e18), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e18)\n",
    "    \n",
    "    resGRU_u_e18.append(u)\n",
    "    resGRU_pred_e18.append(pred_GRU_u)\n",
    "    resGRU_real_e18.append(y_test_e18)\n",
    "    resGRU_modelos_e18.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e18['UNITS'] = resGRU_u_e18\n",
    "resultados_GRU_e18['PREDICCIONES'] = resGRU_pred_e18\n",
    "resultados_GRU_e18['REALES'] = resGRU_real_e18\n",
    "resultados_GRU_e18['RED'] = ['GRU']*len(resGRU_u_e18)\n",
    "resultados_GRU_e18['MODELO'] = resGRU_modelos_e18\n",
    "\n",
    "df_resultados_GRU_e18 = pd.DataFrame(resultados_GRU_e18)\n",
    "evaluacion_e18 = pd.concat([evaluacion_e18, df_resultados_GRU_e18])\n",
    "evaluacion_e18 = evaluacion_e18.reset_index(drop=True)\n",
    "evaluacion_e18.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9520ed",
   "metadata": {},
   "source": [
    "#### 3.4. Estación 24 <a class=\"anchor\" id=\"3.4.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1492281",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e24 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e24 = {}\n",
    "resLSTM_u_e24, resLSTM_pred_e24, resLSTM_real_e24, resLSTM_modelos_e24 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e24.shape[1], X_train_e24.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e24, y_train_e24, validation_data=(X_val_e24, y_val_e24), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e24)\n",
    "    \n",
    "    resLSTM_u_e24.append(u)\n",
    "    resLSTM_pred_e24.append(pred_lstm_u)\n",
    "    resLSTM_real_e24.append(y_test_e24)\n",
    "    resLSTM_modelos_e24.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e24['UNITS'] = resLSTM_u_e24\n",
    "resultados_LSTM_e24['PREDICCIONES'] = resLSTM_pred_e24\n",
    "resultados_LSTM_e24['REALES'] = resLSTM_real_e24\n",
    "resultados_LSTM_e24['RED'] = ['LSTM']*len(resLSTM_u_e24)\n",
    "resultados_LSTM_e24['MODELO'] = resLSTM_modelos_e24\n",
    "\n",
    "df_resultados_LSTM_e24 = pd.DataFrame(resultados_LSTM_e24)\n",
    "evaluacion_e24 = pd.concat([evaluacion_e24, df_resultados_LSTM_e24])\n",
    "\n",
    "resultados_GRU_e24 = {}\n",
    "resGRU_u_e24, resGRU_pred_e24, resGRU_real_e24, resGRU_modelos_e24 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e24.shape[1], X_train_e24.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e24, y_train_e24, validation_data=(X_val_e24, y_val_e24), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e24)\n",
    "    \n",
    "    resGRU_u_e24.append(u)\n",
    "    resGRU_pred_e24.append(pred_GRU_u)\n",
    "    resGRU_real_e24.append(y_test_e24)\n",
    "    resGRU_modelos_e24.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e24['UNITS'] = resGRU_u_e24\n",
    "resultados_GRU_e24['PREDICCIONES'] = resGRU_pred_e24\n",
    "resultados_GRU_e24['REALES'] = resGRU_real_e24\n",
    "resultados_GRU_e24['RED'] = ['GRU']*len(resGRU_u_e24)\n",
    "resultados_GRU_e24['MODELO'] = resGRU_modelos_e24\n",
    "\n",
    "df_resultados_GRU_e24 = pd.DataFrame(resultados_GRU_e24)\n",
    "evaluacion_e24 = pd.concat([evaluacion_e24, df_resultados_GRU_e24])\n",
    "evaluacion_e24 = evaluacion_e24.reset_index(drop=True)\n",
    "evaluacion_e24.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d06e5",
   "metadata": {},
   "source": [
    "#### 3.5. Estación 36 <a class=\"anchor\" id=\"3.5.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e36 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e36 = {}\n",
    "resLSTM_u_e36, resLSTM_pred_e36, resLSTM_real_e36, resLSTM_modelos_e36 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e36.shape[1], X_train_e36.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e36, y_train_e36, validation_data=(X_val_e36, y_val_e36), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e36)\n",
    "    \n",
    "    resLSTM_u_e36.append(u)\n",
    "    resLSTM_pred_e36.append(pred_lstm_u)\n",
    "    resLSTM_real_e36.append(y_test_e36)\n",
    "    resLSTM_modelos_e36.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e36['UNITS'] = resLSTM_u_e36\n",
    "resultados_LSTM_e36['PREDICCIONES'] = resLSTM_pred_e36\n",
    "resultados_LSTM_e36['REALES'] = resLSTM_real_e36\n",
    "resultados_LSTM_e36['RED'] = ['LSTM']*len(resLSTM_u_e36)\n",
    "resultados_LSTM_e36['MODELO'] = resLSTM_modelos_e36\n",
    "\n",
    "df_resultados_LSTM_e36 = pd.DataFrame(resultados_LSTM_e36)\n",
    "evaluacion_e36 = pd.concat([evaluacion_e36, df_resultados_LSTM_e36])\n",
    "\n",
    "resultados_GRU_e36 = {}\n",
    "resGRU_u_e36, resGRU_pred_e36, resGRU_real_e36, resGRU_modelos_e36 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e36.shape[1], X_train_e36.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e36, y_train_e36, validation_data=(X_val_e36, y_val_e36), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e36)\n",
    "    \n",
    "    resGRU_u_e36.append(u)\n",
    "    resGRU_pred_e36.append(pred_GRU_u)\n",
    "    resGRU_real_e36.append(y_test_e36)\n",
    "    resGRU_modelos_e36.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e36['UNITS'] = resGRU_u_e36\n",
    "resultados_GRU_e36['PREDICCIONES'] = resGRU_pred_e36\n",
    "resultados_GRU_e36['REALES'] = resGRU_real_e36\n",
    "resultados_GRU_e36['RED'] = ['GRU']*len(resGRU_u_e36)\n",
    "resultados_GRU_e36['MODELO'] = resGRU_modelos_e36\n",
    "\n",
    "df_resultados_GRU_e36 = pd.DataFrame(resultados_GRU_e36)\n",
    "evaluacion_e36 = pd.concat([evaluacion_e36, df_resultados_GRU_e36])\n",
    "evaluacion_e36 = evaluacion_e36.reset_index(drop=True)\n",
    "evaluacion_e36.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abede2a0",
   "metadata": {},
   "source": [
    "#### 3.6. Estación 38 <a class=\"anchor\" id=\"3.6.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee847c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluacion_e38 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e38 = {}\n",
    "resLSTM_u_e38, resLSTM_pred_e38, resLSTM_real_e38, resLSTM_modelos_e38 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e38.shape[1], X_train_e38.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e38, y_train_e38, validation_data=(X_val_e38, y_val_e38), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e38)\n",
    "    \n",
    "    resLSTM_u_e38.append(u)\n",
    "    resLSTM_pred_e38.append(pred_lstm_u)\n",
    "    resLSTM_real_e38.append(y_test_e38)\n",
    "    resLSTM_modelos_e38.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e38['UNITS'] = resLSTM_u_e38\n",
    "resultados_LSTM_e38['PREDICCIONES'] = resLSTM_pred_e38\n",
    "resultados_LSTM_e38['REALES'] = resLSTM_real_e38\n",
    "resultados_LSTM_e38['RED'] = ['LSTM']*len(resLSTM_u_e38)\n",
    "resultados_LSTM_e38['MODELO'] = resLSTM_modelos_e38\n",
    "\n",
    "df_resultados_LSTM_e38 = pd.DataFrame(resultados_LSTM_e38)\n",
    "evaluacion_e38 = pd.concat([evaluacion_e38, df_resultados_LSTM_e38])\n",
    "\n",
    "resultados_GRU_e38 = {}\n",
    "resGRU_u_e38, resGRU_pred_e38, resGRU_real_e38, resGRU_modelos_e38 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e38.shape[1], X_train_e38.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e38, y_train_e38, validation_data=(X_val_e38, y_val_e38), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e38)\n",
    "    \n",
    "    resGRU_u_e38.append(u)\n",
    "    resGRU_pred_e38.append(pred_GRU_u)\n",
    "    resGRU_real_e38.append(y_test_e38)\n",
    "    resGRU_modelos_e38.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e38['UNITS'] = resGRU_u_e38\n",
    "resultados_GRU_e38['PREDICCIONES'] = resGRU_pred_e38\n",
    "resultados_GRU_e38['REALES'] = resGRU_real_e38\n",
    "resultados_GRU_e38['RED'] = ['GRU']*len(resGRU_u_e38)\n",
    "resultados_GRU_e38['MODELO'] = resGRU_modelos_e38\n",
    "\n",
    "df_resultados_GRU_e38 = pd.DataFrame(resultados_GRU_e38)\n",
    "evaluacion_e38 = pd.concat([evaluacion_e38, df_resultados_GRU_e38])\n",
    "evaluacion_e38 = evaluacion_e38.reset_index(drop=True)\n",
    "evaluacion_e38.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf91c5b",
   "metadata": {},
   "source": [
    "#### 3.7. Estación 40 <a class=\"anchor\" id=\"3.7.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e40 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e40 = {}\n",
    "resLSTM_u_e40, resLSTM_pred_e40, resLSTM_real_e40, resLSTM_modelos_e40 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e40.shape[1], X_train_e40.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e40, y_train_e40, validation_data=(X_val_e40, y_val_e40), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e40)\n",
    "    \n",
    "    resLSTM_u_e40.append(u)\n",
    "    resLSTM_pred_e40.append(pred_lstm_u)\n",
    "    resLSTM_real_e40.append(y_test_e40)\n",
    "    resLSTM_modelos_e40.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e40['UNITS'] = resLSTM_u_e40\n",
    "resultados_LSTM_e40['PREDICCIONES'] = resLSTM_pred_e40\n",
    "resultados_LSTM_e40['REALES'] = resLSTM_real_e40\n",
    "resultados_LSTM_e40['RED'] = ['LSTM']*len(resLSTM_u_e40)\n",
    "resultados_LSTM_e40['MODELO'] = resLSTM_modelos_e40\n",
    "\n",
    "df_resultados_LSTM_e40 = pd.DataFrame(resultados_LSTM_e40)\n",
    "evaluacion_e40 = pd.concat([evaluacion_e40, df_resultados_LSTM_e40])\n",
    "\n",
    "resultados_GRU_e40 = {}\n",
    "resGRU_u_e40, resGRU_pred_e40, resGRU_real_e40, resGRU_modelos_e40 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e40.shape[1], X_train_e40.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e40, y_train_e40, validation_data=(X_val_e40, y_val_e40), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e40)\n",
    "    \n",
    "    resGRU_u_e40.append(u)\n",
    "    resGRU_pred_e40.append(pred_GRU_u)\n",
    "    resGRU_real_e40.append(y_test_e40)\n",
    "    resGRU_modelos_e40.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e40['UNITS'] = resGRU_u_e40\n",
    "resultados_GRU_e40['PREDICCIONES'] = resGRU_pred_e40\n",
    "resultados_GRU_e40['REALES'] = resGRU_real_e40\n",
    "resultados_GRU_e40['RED'] = ['GRU']*len(resGRU_u_e40)\n",
    "resultados_GRU_e40['MODELO'] = resGRU_modelos_e40\n",
    "\n",
    "df_resultados_GRU_e40 = pd.DataFrame(resultados_GRU_e40)\n",
    "evaluacion_e40 = pd.concat([evaluacion_e40, df_resultados_GRU_e40])\n",
    "evaluacion_e40 = evaluacion_e40.reset_index(drop=True)\n",
    "evaluacion_e40.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53351dfb",
   "metadata": {},
   "source": [
    "#### 3.8. Estación 47 <a class=\"anchor\" id=\"3.8.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e47 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e47 = {}\n",
    "resLSTM_u_e47, resLSTM_pred_e47, resLSTM_real_e47, resLSTM_modelos_e47 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e47.shape[1], X_train_e47.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e47, y_train_e47, validation_data=(X_val_e47, y_val_e47), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e47)\n",
    "    \n",
    "    resLSTM_u_e47.append(u)\n",
    "    resLSTM_pred_e47.append(pred_lstm_u)\n",
    "    resLSTM_real_e47.append(y_test_e47)\n",
    "    resLSTM_modelos_e47.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e47['UNITS'] = resLSTM_u_e47\n",
    "resultados_LSTM_e47['PREDICCIONES'] = resLSTM_pred_e47\n",
    "resultados_LSTM_e47['REALES'] = resLSTM_real_e47\n",
    "resultados_LSTM_e47['RED'] = ['LSTM']*len(resLSTM_u_e47)\n",
    "resultados_LSTM_e47['MODELO'] = resLSTM_modelos_e47\n",
    "\n",
    "df_resultados_LSTM_e47 = pd.DataFrame(resultados_LSTM_e47)\n",
    "evaluacion_e47 = pd.concat([evaluacion_e47, df_resultados_LSTM_e47])\n",
    "\n",
    "resultados_GRU_e47 = {}\n",
    "resGRU_u_e47, resGRU_pred_e47, resGRU_real_e47, resGRU_modelos_e47 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e47.shape[1], X_train_e47.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e47, y_train_e47, validation_data=(X_val_e47, y_val_e47), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e47)\n",
    "    \n",
    "    resGRU_u_e47.append(u)\n",
    "    resGRU_pred_e47.append(pred_GRU_u)\n",
    "    resGRU_real_e47.append(y_test_e47)\n",
    "    resGRU_modelos_e47.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e47['UNITS'] = resGRU_u_e47\n",
    "resultados_GRU_e47['PREDICCIONES'] = resGRU_pred_e47\n",
    "resultados_GRU_e47['REALES'] = resGRU_real_e47\n",
    "resultados_GRU_e47['RED'] = ['GRU']*len(resGRU_u_e47)\n",
    "resultados_GRU_e47['MODELO'] = resGRU_modelos_e47\n",
    "\n",
    "df_resultados_GRU_e47 = pd.DataFrame(resultados_GRU_e47)\n",
    "evaluacion_e47 = pd.concat([evaluacion_e47, df_resultados_GRU_e47])\n",
    "evaluacion_e47 = evaluacion_e47.reset_index(drop=True)\n",
    "evaluacion_e47.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33f048",
   "metadata": {},
   "source": [
    "#### 3.9. Estación 48 <a class=\"anchor\" id=\"3.9.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ed5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e48 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e48 = {}\n",
    "resLSTM_u_e48, resLSTM_pred_e48, resLSTM_real_e48, resLSTM_modelos_e48 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e48.shape[1], X_train_e48.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e48, y_train_e48, validation_data=(X_val_e48, y_val_e48), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e48)\n",
    "    \n",
    "    resLSTM_u_e48.append(u)\n",
    "    resLSTM_pred_e48.append(pred_lstm_u)\n",
    "    resLSTM_real_e48.append(y_test_e48)\n",
    "    resLSTM_modelos_e48.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e48['UNITS'] = resLSTM_u_e48\n",
    "resultados_LSTM_e48['PREDICCIONES'] = resLSTM_pred_e48\n",
    "resultados_LSTM_e48['REALES'] = resLSTM_real_e48\n",
    "resultados_LSTM_e48['RED'] = ['LSTM']*len(resLSTM_u_e48)\n",
    "resultados_LSTM_e48['MODELO'] = resLSTM_modelos_e48\n",
    "\n",
    "df_resultados_LSTM_e48 = pd.DataFrame(resultados_LSTM_e48)\n",
    "evaluacion_e48 = pd.concat([evaluacion_e48, df_resultados_LSTM_e48])\n",
    "\n",
    "resultados_GRU_e48 = {}\n",
    "resGRU_u_e48, resGRU_pred_e48, resGRU_real_e48, resGRU_modelos_e48 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e48.shape[1], X_train_e48.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e48, y_train_e48, validation_data=(X_val_e48, y_val_e48), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e48)\n",
    "    \n",
    "    resGRU_u_e48.append(u)\n",
    "    resGRU_pred_e48.append(pred_GRU_u)\n",
    "    resGRU_real_e48.append(y_test_e48)\n",
    "    resGRU_modelos_e48.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e48['UNITS'] = resGRU_u_e48\n",
    "resultados_GRU_e48['PREDICCIONES'] = resGRU_pred_e48\n",
    "resultados_GRU_e48['REALES'] = resGRU_real_e48\n",
    "resultados_GRU_e48['RED'] = ['GRU']*len(resGRU_u_e48)\n",
    "resultados_GRU_e48['MODELO'] = resGRU_modelos_e48\n",
    "\n",
    "df_resultados_GRU_e48 = pd.DataFrame(resultados_GRU_e48)\n",
    "evaluacion_e48 = pd.concat([evaluacion_e48, df_resultados_GRU_e48])\n",
    "evaluacion_e48 = evaluacion_e48.reset_index(drop=True)\n",
    "evaluacion_e48.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b61d9e",
   "metadata": {},
   "source": [
    "#### 3.10. Estación 50 <a class=\"anchor\" id=\"3.10.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e50 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e50 = {}\n",
    "resLSTM_u_e50, resLSTM_pred_e50, resLSTM_real_e50, resLSTM_modelos_e50 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e50.shape[1], X_train_e50.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e50, y_train_e50, validation_data=(X_val_e50, y_val_e50), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e50)\n",
    "    \n",
    "    resLSTM_u_e50.append(u)\n",
    "    resLSTM_pred_e50.append(pred_lstm_u)\n",
    "    resLSTM_real_e50.append(y_test_e50)\n",
    "    resLSTM_modelos_e50.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e50['UNITS'] = resLSTM_u_e50\n",
    "resultados_LSTM_e50['PREDICCIONES'] = resLSTM_pred_e50\n",
    "resultados_LSTM_e50['REALES'] = resLSTM_real_e50\n",
    "resultados_LSTM_e50['RED'] = ['LSTM']*len(resLSTM_u_e50)\n",
    "resultados_LSTM_e50['MODELO'] = resLSTM_modelos_e50\n",
    "\n",
    "df_resultados_LSTM_e50 = pd.DataFrame(resultados_LSTM_e50)\n",
    "evaluacion_e50 = pd.concat([evaluacion_e50, df_resultados_LSTM_e50])\n",
    "\n",
    "resultados_GRU_e50 = {}\n",
    "resGRU_u_e50, resGRU_pred_e50, resGRU_real_e50, resGRU_modelos_e50 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e50.shape[1], X_train_e50.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e50, y_train_e50, validation_data=(X_val_e50, y_val_e50), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e50)\n",
    "    \n",
    "    resGRU_u_e50.append(u)\n",
    "    resGRU_pred_e50.append(pred_GRU_u)\n",
    "    resGRU_real_e50.append(y_test_e50)\n",
    "    resGRU_modelos_e50.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e50['UNITS'] = resGRU_u_e50\n",
    "resultados_GRU_e50['PREDICCIONES'] = resGRU_pred_e50\n",
    "resultados_GRU_e50['REALES'] = resGRU_real_e50\n",
    "resultados_GRU_e50['RED'] = ['GRU']*len(resGRU_u_e50)\n",
    "resultados_GRU_e50['MODELO'] = resGRU_modelos_e50\n",
    "\n",
    "df_resultados_GRU_e50 = pd.DataFrame(resultados_GRU_e50)\n",
    "evaluacion_e50 = pd.concat([evaluacion_e50, df_resultados_GRU_e50])\n",
    "evaluacion_e50 = evaluacion_e50.reset_index(drop=True)\n",
    "evaluacion_e50.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c456eb2",
   "metadata": {},
   "source": [
    "#### 3.11. Estación 55 <a class=\"anchor\" id=\"3.11.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e55 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e55 = {}\n",
    "resLSTM_u_e55, resLSTM_pred_e55, resLSTM_real_e55, resLSTM_modelos_e55 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e55.shape[1], X_train_e55.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e55, y_train_e55, validation_data=(X_val_e55, y_val_e55), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e55)\n",
    "    \n",
    "    resLSTM_u_e55.append(u)\n",
    "    resLSTM_pred_e55.append(pred_lstm_u)\n",
    "    resLSTM_real_e55.append(y_test_e55)\n",
    "    resLSTM_modelos_e55.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e55['UNITS'] = resLSTM_u_e55\n",
    "resultados_LSTM_e55['PREDICCIONES'] = resLSTM_pred_e55\n",
    "resultados_LSTM_e55['REALES'] = resLSTM_real_e55\n",
    "resultados_LSTM_e55['RED'] = ['LSTM']*len(resLSTM_u_e55)\n",
    "resultados_LSTM_e55['MODELO'] = resLSTM_modelos_e55\n",
    "\n",
    "df_resultados_LSTM_e55 = pd.DataFrame(resultados_LSTM_e55)\n",
    "evaluacion_e55 = pd.concat([evaluacion_e55, df_resultados_LSTM_e55])\n",
    "\n",
    "resultados_GRU_e55 = {}\n",
    "resGRU_u_e55, resGRU_pred_e55, resGRU_real_e55, resGRU_modelos_e55 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e55.shape[1], X_train_e55.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e55, y_train_e55, validation_data=(X_val_e55, y_val_e55), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e55)\n",
    "    \n",
    "    resGRU_u_e55.append(u)\n",
    "    resGRU_pred_e55.append(pred_GRU_u)\n",
    "    resGRU_real_e55.append(y_test_e55)\n",
    "    resGRU_modelos_e55.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e55['UNITS'] = resGRU_u_e55\n",
    "resultados_GRU_e55['PREDICCIONES'] = resGRU_pred_e55\n",
    "resultados_GRU_e55['REALES'] = resGRU_real_e55\n",
    "resultados_GRU_e55['RED'] = ['GRU']*len(resGRU_u_e55)\n",
    "resultados_GRU_e55['MODELO'] = resGRU_modelos_e55\n",
    "\n",
    "df_resultados_GRU_e55 = pd.DataFrame(resultados_GRU_e55)\n",
    "evaluacion_e55 = pd.concat([evaluacion_e55, df_resultados_GRU_e55])\n",
    "evaluacion_e55 = evaluacion_e55.reset_index(drop=True)\n",
    "evaluacion_e55.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821e33b",
   "metadata": {},
   "source": [
    "#### 3.12. Estación 57 <a class=\"anchor\" id=\"3.12.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8388a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e57 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e57 = {}\n",
    "resLSTM_u_e57, resLSTM_pred_e57, resLSTM_real_e57, resLSTM_modelos_e57 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e57.shape[1], X_train_e57.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e57, y_train_e57, validation_data=(X_val_e57, y_val_e57), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e57)\n",
    "    \n",
    "    resLSTM_u_e57.append(u)\n",
    "    resLSTM_pred_e57.append(pred_lstm_u)\n",
    "    resLSTM_real_e57.append(y_test_e57)\n",
    "    resLSTM_modelos_e57.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e57['UNITS'] = resLSTM_u_e57\n",
    "resultados_LSTM_e57['PREDICCIONES'] = resLSTM_pred_e57\n",
    "resultados_LSTM_e57['REALES'] = resLSTM_real_e57\n",
    "resultados_LSTM_e57['RED'] = ['LSTM']*len(resLSTM_u_e57)\n",
    "resultados_LSTM_e57['MODELO'] = resLSTM_modelos_e57\n",
    "\n",
    "df_resultados_LSTM_e57 = pd.DataFrame(resultados_LSTM_e57)\n",
    "evaluacion_e57 = pd.concat([evaluacion_e57, df_resultados_LSTM_e57])\n",
    "\n",
    "resultados_GRU_e57 = {}\n",
    "resGRU_u_e57, resGRU_pred_e57, resGRU_real_e57, resGRU_modelos_e57 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e57.shape[1], X_train_e57.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e57, y_train_e57, validation_data=(X_val_e57, y_val_e57), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e57)\n",
    "    \n",
    "    resGRU_u_e57.append(u)\n",
    "    resGRU_pred_e57.append(pred_GRU_u)\n",
    "    resGRU_real_e57.append(y_test_e57)\n",
    "    resGRU_modelos_e57.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e57['UNITS'] = resGRU_u_e57\n",
    "resultados_GRU_e57['PREDICCIONES'] = resGRU_pred_e57\n",
    "resultados_GRU_e57['REALES'] = resGRU_real_e57\n",
    "resultados_GRU_e57['RED'] = ['GRU']*len(resGRU_u_e57)\n",
    "resultados_GRU_e57['MODELO'] = resGRU_modelos_e57\n",
    "\n",
    "df_resultados_GRU_e57 = pd.DataFrame(resultados_GRU_e57)\n",
    "evaluacion_e57 = pd.concat([evaluacion_e57, df_resultados_GRU_e57])\n",
    "evaluacion_e57 = evaluacion_e57.reset_index(drop=True)\n",
    "evaluacion_e57.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9a046",
   "metadata": {},
   "source": [
    "#### 3.13. Estación 60 <a class=\"anchor\" id=\"3.13.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluacion_e60 = pd.DataFrame()\n",
    "\n",
    "units = [32, 64, 100]\n",
    "\n",
    "resultados_LSTM_e60 = {}\n",
    "resLSTM_u_e60, resLSTM_pred_e60, resLSTM_real_e60, resLSTM_modelos_e60 = [], [], [], []\n",
    "\n",
    "print('LSTM')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(X_train_e60.shape[1], X_train_e60.shape[2])),\n",
    "        LSTM(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    history_model_lstm = model_lstm.fit(X_train_e60, y_train_e60, validation_data=(X_val_e60, y_val_e60), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_lstm_u = model_lstm.predict(X_test_e60)\n",
    "    \n",
    "    resLSTM_u_e60.append(u)\n",
    "    resLSTM_pred_e60.append(pred_lstm_u)\n",
    "    resLSTM_real_e60.append(y_test_e60)\n",
    "    resLSTM_modelos_e60.append(model_lstm)\n",
    "    \n",
    "resultados_LSTM_e60['UNITS'] = resLSTM_u_e60\n",
    "resultados_LSTM_e60['PREDICCIONES'] = resLSTM_pred_e60\n",
    "resultados_LSTM_e60['REALES'] = resLSTM_real_e60\n",
    "resultados_LSTM_e60['RED'] = ['LSTM']*len(resLSTM_u_e60)\n",
    "resultados_LSTM_e60['MODELO'] = resLSTM_modelos_e60\n",
    "\n",
    "df_resultados_LSTM_e60 = pd.DataFrame(resultados_LSTM_e60)\n",
    "evaluacion_e60 = pd.concat([evaluacion_e60, df_resultados_LSTM_e60])\n",
    "\n",
    "resultados_GRU_e60 = {}\n",
    "resGRU_u_e60, resGRU_pred_e60, resGRU_real_e60, resGRU_modelos_e60 = [], [], [], []\n",
    "\n",
    "print('GRU')\n",
    "for u in units:\n",
    "    print('\\tConfiguracion:', u)\n",
    "    model_GRU = Sequential([\n",
    "        Input(shape=(X_train_e60.shape[1], X_train_e60.shape[2])),\n",
    "        GRU(units=u),  \n",
    "        Dense(units=48)])\n",
    "    model_GRU.compile(loss='mse', optimizer='adam')\n",
    "    history_model_GRU = model_GRU.fit(X_train_e60, y_train_e60, validation_data=(X_val_e60, y_val_e60), epochs=1, batch_size=32, verbose=False)\n",
    "    pred_GRU_u = model_GRU.predict(X_test_e60)\n",
    "    \n",
    "    resGRU_u_e60.append(u)\n",
    "    resGRU_pred_e60.append(pred_GRU_u)\n",
    "    resGRU_real_e60.append(y_test_e60)\n",
    "    resGRU_modelos_e60.append(model_GRU)\n",
    "    \n",
    "resultados_GRU_e60['UNITS'] = resGRU_u_e60\n",
    "resultados_GRU_e60['PREDICCIONES'] = resGRU_pred_e60\n",
    "resultados_GRU_e60['REALES'] = resGRU_real_e60\n",
    "resultados_GRU_e60['RED'] = ['GRU']*len(resGRU_u_e60)\n",
    "resultados_GRU_e60['MODELO'] = resGRU_modelos_e60\n",
    "\n",
    "df_resultados_GRU_e60 = pd.DataFrame(resultados_GRU_e60)\n",
    "evaluacion_e60 = pd.concat([evaluacion_e60, df_resultados_GRU_e60])\n",
    "evaluacion_e60 = evaluacion_e60.reset_index(drop=True)\n",
    "evaluacion_e60.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05889bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluaciones = pd.concat([evaluacion_e8,\n",
    "                        evaluacion_e18,\n",
    "                        evaluacion_e24,\n",
    "                        evaluacion_e36,\n",
    "                        evaluacion_e38,\n",
    "                        evaluacion_e40,\n",
    "                        evaluacion_e47,\n",
    "                        evaluacion_e48,\n",
    "                        evaluacion_e50,\n",
    "                        evaluacion_e55,\n",
    "                        evaluacion_e57,\n",
    "                        evaluacion_e60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluaciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluaciones.to_csv('evaluaciones_modelos_TFM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fbbac7",
   "metadata": {},
   "source": [
    "### 4. Análisis de resultados <a class=\"anchor\" id=\"4.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "p = evaluaciones.copy()\n",
    "col_est = ['8']*6+['18']*6+['24']*6+['36']*6+['38']*6+['40']*6+['47']*6+['48']*6+['50']*6+['55']*6+['57']*6+['60']*6\n",
    "p['ESTACION'] = col_est\n",
    "p['MODELO'] = p['RED'] + ' ' + p['UNITS'].astype(str)\n",
    "\n",
    "dfs = []\n",
    "for station, pred_naive, y_test in [(8, preds_naive['e_8'], y_test_e8), \n",
    "                                    (18, preds_naive['e_18'], y_test_e18),\n",
    "                                    (24, preds_naive['e_24'], y_test_e24),\n",
    "                                    (36, preds_naive['e_36'], y_test_e36),\n",
    "                                    (38, preds_naive['e_38'], y_test_e38), \n",
    "                                    (40, preds_naive['e_40'], y_test_e40),\n",
    "                                    (47, preds_naive['e_47'], y_test_e47), \n",
    "                                    (48, preds_naive['e_48'], y_test_e48),\n",
    "                                    (50, preds_naive['e_50'], y_test_e50), \n",
    "                                    (55, preds_naive['e_55'], y_test_e55),\n",
    "                                    (57, preds_naive['e_57'], y_test_e57), \n",
    "                                    (60, preds_naive['e_60'], y_test_e60)]:\n",
    "    df = pd.DataFrame({'UNITS': [''], 'PREDICCIONES': [pred_naive], 'REALES': [y_test], 'RED': ['Naive'], 'MODELO': ['Naive'], 'ESTACION': [str(station)]})\n",
    "    dfs.append(df)\n",
    "\n",
    "for station, pred_naive, y_test in [(8, preds_media['e_8'], y_test_e8), \n",
    "                                    (18, preds_media['e_18'], y_test_e18),\n",
    "                                    (24, preds_media['e_24'], y_test_e24),\n",
    "                                    (36, preds_media['e_36'], y_test_e36),\n",
    "                                    (38, preds_media['e_38'], y_test_e38), \n",
    "                                    (40, preds_media['e_40'], y_test_e40),\n",
    "                                    (47, preds_media['e_47'], y_test_e47), \n",
    "                                    (48, preds_media['e_48'], y_test_e48),\n",
    "                                    (50, preds_media['e_50'], y_test_e50), \n",
    "                                    (55, preds_media['e_55'], y_test_e55),\n",
    "                                    (57, preds_media['e_57'], y_test_e57), \n",
    "                                    (60, preds_media['e_60'], y_test_e60)]:\n",
    "    df = pd.DataFrame({'UNITS': [''], 'PREDICCIONES': [pred_naive], 'REALES': [y_test], 'RED': ['Media'], 'MODELO': ['Media'], 'ESTACION': [str(station)]})\n",
    "    dfs.append(df)\n",
    "    \n",
    "for station, pred_naive, y_test in [(8, preds_48hrs['e_8'], y_test_e8), \n",
    "                                    (18, preds_48hrs['e_18'], y_test_e18),\n",
    "                                    (24, preds_48hrs['e_24'], y_test_e24),\n",
    "                                    (36, preds_48hrs['e_36'], y_test_e36),\n",
    "                                    (38, preds_48hrs['e_38'], y_test_e38), \n",
    "                                    (40, preds_48hrs['e_40'], y_test_e40),\n",
    "                                    (47, preds_48hrs['e_47'], y_test_e47), \n",
    "                                    (48, preds_48hrs['e_48'], y_test_e48),\n",
    "                                    (50, preds_48hrs['e_50'], y_test_e50), \n",
    "                                    (55, preds_48hrs['e_55'], y_test_e55),\n",
    "                                    (57, preds_48hrs['e_57'], y_test_e57), \n",
    "                                    (60, preds_48hrs['e_60'], y_test_e60)]:\n",
    "    df = pd.DataFrame({'UNITS': [''], 'PREDICCIONES': [pred_naive], 'REALES': [y_test], 'RED': ['Steps'], 'MODELO': ['Steps'], 'ESTACION': [str(station)]})\n",
    "    dfs.append(df)\n",
    "\n",
    "p = pd.concat([p] + dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "p['ESTACION_n'] = 'Estacion ' + p['ESTACION']\n",
    "\n",
    "anyadir = []\n",
    "anyadir_2 = []\n",
    "r2_values = []\n",
    "\n",
    "for i in range(len(p)):\n",
    "    row = p.iloc[i]\n",
    "    pred = row['PREDICCIONES']\n",
    "    real = row['REALES']\n",
    "    \n",
    "    rmse = np.sqrt(((pred - real) ** 2))\n",
    "    rmse_2 = np.sqrt(np.mean((pred - real) ** 2))\n",
    "\n",
    "    r2 = r2_score(real, pred)\n",
    "    anyadir.append(rmse)\n",
    "    anyadir_2.append(rmse_2)\n",
    "    r2_values.append(r2)\n",
    "\n",
    "p['RMSE_ind'] = anyadir\n",
    "p['RMSE_med'] = anyadir_2\n",
    "p['R2'] = r2_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc34e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['MODELO'] = p['RED'] + ' ' + p['UNITS'].astype(str)\n",
    "p['ESTACION_n'] = 'Estacion '+p['ESTACION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f21ad",
   "metadata": {},
   "source": [
    "#### 4.1. Medias RMSE y R2 por modelo <a class=\"anchor\" id=\"4.1.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_rmse_por_modelo = p.groupby('MODELO')['RMSE_med'].mean()\n",
    "media_rmse_por_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_r2_por_modelo = p.groupby('MODELO')['R2'].mean()\n",
    "media_r2_por_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = media_rmse_por_modelo.index\n",
    "rmse_med = media_rmse_por_modelo.values\n",
    "\n",
    "colores_gru = ['darkslategray', 'darkslategray', 'darkslategray']\n",
    "colores_lstm = ['coral', 'coral', 'coral']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4.5))\n",
    "\n",
    "for i, modelo in enumerate(modelos[:3]):\n",
    "    ax.barh(modelo, rmse_med[i], color=colores_gru[i], label=modelo)\n",
    "    ax.text(rmse_med[i]+0.1, i, '{:.2f}'.format(rmse_med[i]), ha='left', va='center')\n",
    "\n",
    "for i, modelo in enumerate(modelos[3:6]):\n",
    "    ax.barh(modelo, rmse_med[i+3], color=colores_lstm[i], label=modelo)\n",
    "    ax.text(rmse_med[i+3]+0.1, i+3, '{:.2f}'.format(rmse_med[i+3]), ha='left', va='center')\n",
    "\n",
    "ax.barh(modelos[-2], rmse_med[-2], color='lightgoldenrodyellow', label=modelos[-2])\n",
    "ax.text(rmse_med[-2]+0.1, 6, '{:.2f}'.format(rmse_med[-2]), ha='left', va='center')\n",
    "\n",
    "ax.barh(modelos[-1], rmse_med[-1], color='lightgoldenrodyellow', label=modelos[-1])\n",
    "ax.text(rmse_med[-1]+0.1, 7, '{:.2f}'.format(rmse_med[-1]), ha='left', va='center')\n",
    "\n",
    "ax.set_xlabel('RMSE')\n",
    "ax.set_ylabel('Modelo')\n",
    "ax.set_title('RMSE por Modelo')\n",
    "\n",
    "ax.legend().remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = media_r2_por_modelo.index\n",
    "r2_med = media_r2_por_modelo.values\n",
    "\n",
    "def posicion_etiqueta(valor, factor=0.005, minimo=0.01):\n",
    "    return valor + max(valor * factor, minimo)\n",
    "\n",
    "colores_gru = ['darkslategray', 'darkslategray', 'darkslategray']\n",
    "colores_lstm = ['darkcyan', 'darkcyan', 'darkcyan']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4.5))\n",
    "\n",
    "for i, modelo in enumerate(modelos[:3]):\n",
    "    ax.barh(modelo, r2_med[i], color=colores_gru[i], label=modelo)\n",
    "    ax.text(posicion_etiqueta(r2_med[i]), i, '{:.2f}'.format(r2_med[i]), ha='left', va='center')\n",
    "\n",
    "for i, modelo in enumerate(modelos[3:6]):\n",
    "    ax.barh(modelo, r2_med[i+3], color=colores_lstm[i], label=modelo)\n",
    "    ax.text(posicion_etiqueta(r2_med[i+3]), i+3, '{:.2f}'.format(r2_med[i+3]), ha='left', va='center')\n",
    "\n",
    "ax.barh(modelos[-1], r2_med[-1], color='powderblue', label=modelos[-1])\n",
    "ax.text(posicion_etiqueta(r2_med[-1]), 6, '{:.2f}'.format(r2_med[-1]), ha='left', va='center')\n",
    "\n",
    "# Ajustar el diseño\n",
    "ax.set_xlabel('R2')\n",
    "ax.set_ylabel('Modelo')\n",
    "ax.set_title('R2 por Modelo')\n",
    "\n",
    "# Eliminar leyenda\n",
    "ax.legend().remove()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f3e98",
   "metadata": {},
   "source": [
    "#### 4.2. Contrastes DM unilaterales <a class=\"anchor\" id=\"4.2.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560abfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from dieboldmariano import dm_test, ZeroVarianceException\n",
    "d =  {'ESTACION':[], 'MODELO 1':[], 'MODELO 2':[], 'DM':[], 'PVALUE':[], 'DIFERENCIA':[]}\n",
    "for e in p['ESTACION'].unique():\n",
    "    p_e = p[p['ESTACION']==e]\n",
    "    print(p_e['ESTACION_n'].unique()[0])\n",
    "    print()\n",
    "    for i in range(len(p_e)):\n",
    "        for j in range(len(p_e)):\n",
    "            if i != j:\n",
    "                m1 = p_e.iloc[i]\n",
    "                m2 = p_e.iloc[j]\n",
    "                print('Modelos:')\n",
    "                print('\\tModelo 1: ', m1['MODELO'])\n",
    "                print('\\tModelo 2: ', m2['MODELO'])\n",
    "                DM_stat, p_value = dm_test(m1['REALES'].flatten(), m1['PREDICCIONES'].flatten(), m2['PREDICCIONES'].flatten(),\n",
    "                                          h=48, one_sided=True)\n",
    "                print()\n",
    "                print()\n",
    "                print(f'Estadístico DM: {DM_stat}, p-valor: {p_value}')\n",
    "                alpha = 0.005\n",
    "                d['ESTACION'].append(p_e['ESTACION_n'])\n",
    "                d['MODELO 1'].append(m1['MODELO'])\n",
    "                d['MODELO 2'].append(m2['MODELO'])\n",
    "                d['DM'].append(DM_stat)\n",
    "                d['PVALUE'].append(p_value)\n",
    "                if p_value < alpha:\n",
    "                    print()\n",
    "                    print(\"Hay una diferencia significativa entre los modelos (rechazamos H0)\")\n",
    "                    d['DIFERENCIA'].append(1)\n",
    "                else:\n",
    "                    print(\"No hay una diferencia significativa entre los modelos (no rechazamos H0)\")\n",
    "                    d['DIFERENCIA'].append(0)\n",
    "                print()\n",
    "\n",
    "df_DMS_uni = pd.DataFrame(data=d)\n",
    "df_DMS_uni['ESTACION'] = df_DMS_uni['ESTACION'].apply(lambda x: x.iloc[0] if isinstance(x, pd.Series) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1338e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DMS_uni.to_csv('df_dms_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4326831",
   "metadata": {},
   "source": [
    "#### 4.3. Contrastes DM bilaterales <a class=\"anchor\" id=\"4.3.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from dieboldmariano import dm_test, ZeroVarianceException\n",
    "d =  {'ESTACION':[], 'MODELO 1':[], 'MODELO 2':[], 'DM':[], 'PVALUE':[], 'DIFERENCIA':[]}\n",
    "for e in p['ESTACION'].unique():\n",
    "    p_e = p[p['ESTACION']==e]\n",
    "    print(p_e['ESTACION_n'].unique()[0])\n",
    "    print()\n",
    "    for i in range(len(p_e)):\n",
    "        for j in range(len(p_e)):\n",
    "            if i != j:\n",
    "                m1 = p_e.iloc[i]\n",
    "                m2 = p_e.iloc[j]\n",
    "                print('Modelos:')\n",
    "                print('\\tModelo 1: ', m1['MODELO'])\n",
    "                print('\\tModelo 2: ', m2['MODELO'])\n",
    "                DM_stat, p_value = dm_test(m1['REALES'].flatten(), m1['PREDICCIONES'].flatten(), m2['PREDICCIONES'].flatten(),\n",
    "                                          h=48, one_sided=False)\n",
    "                print()\n",
    "                print()\n",
    "                print(f'Estadístico DM: {DM_stat}, p-valor: {p_value}')\n",
    "                alpha = 0.005\n",
    "                d['ESTACION'].append(p_e['ESTACION_n'])\n",
    "                d['MODELO 1'].append(m1['MODELO'])\n",
    "                d['MODELO 2'].append(m2['MODELO'])\n",
    "                d['DM'].append(DM_stat)\n",
    "                d['PVALUE'].append(p_value)\n",
    "                if p_value < alpha:\n",
    "                    print()\n",
    "                    print(\"Hay una diferencia significativa entre los modelos (rechazamos H0)\")\n",
    "                    d['DIFERENCIA'].append(1)\n",
    "                else:\n",
    "                    print(\"No hay una diferencia significativa entre los modelos (no rechazamos H0)\")\n",
    "                    d['DIFERENCIA'].append(0)\n",
    "                print()\n",
    "\n",
    "df_DMS_bil = pd.DataFrame(data=d)\n",
    "df_DMS_bil['ESTACION'] = df_DMS_bil['ESTACION'].apply(lambda x: x.iloc[0] if isinstance(x, pd.Series) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DMS_bil.to_csv('df_dms_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c195b2",
   "metadata": {},
   "source": [
    "#### 4.4. Veces que los modelos tienen diferencias significativas <a class=\"anchor\" id=\"4.4.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a902703",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = p['MODELO'].unique()[:-1]\n",
    "tabla = pd.DataFrame(index=modelos, columns=modelos)\n",
    "\n",
    "for modelo1 in modelos:\n",
    "    for modelo2 in modelos:\n",
    "        if modelo1 != modelo2:\n",
    "            suma_dif = df_DMS_bil[(df_DMS_bil['MODELO 1'] == modelo1) & (df_DMS_bil['MODELO 2'] == modelo2)]['DIFERENCIA'].sum()\n",
    "            tabla.loc[modelo1, modelo2] = suma_dif\n",
    "\n",
    "for i in range(len(modelos)):\n",
    "    tabla.iloc[i, i] = '-'\n",
    "\n",
    "print()\n",
    "print(tabla)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ce311",
   "metadata": {},
   "source": [
    "#### 4.5. RMSE y R2 por modelo y estación <a class=\"anchor\" id=\"4.5.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = p['MODELO'].unique()\n",
    "estaciones = p['ESTACION_n'].unique()\n",
    "tabla2 = pd.DataFrame(index=modelos, columns=estaciones)\n",
    "tabla3 = pd.DataFrame(index=modelos, columns=estaciones)\n",
    "for mod in modelos:\n",
    "    for est in estaciones:\n",
    "        tabla2.loc[mod, est] = p[(p['MODELO']==mod)&(p['ESTACION_n']==est)]['RMSE_med'].mean()\n",
    "        tabla3.loc[mod, est] = p[(p['MODELO']==mod)&(p['ESTACION_n']==est)]['R2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = tabla2.T.copy()\n",
    "hm['LSTM 32'] = hm['LSTM 32'].astype(float)\n",
    "hm['LSTM 64'] = hm['LSTM 64'].astype(float)\n",
    "hm['LSTM 100'] = hm['LSTM 100'].astype(float)\n",
    "hm['GRU 32'] = hm['GRU 32'].astype(float)\n",
    "hm['GRU 64'] = hm['GRU 64'].astype(float)\n",
    "hm['GRU 100'] = hm['GRU 100'].astype(float)\n",
    "hm['Naive '] = hm['Naive '].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmr2 = tabla3.T.copy()\n",
    "hmr2['LSTM 32'] = hmr2['LSTM 32'].astype(float)\n",
    "hmr2['LSTM 64'] = hmr2['LSTM 64'].astype(float)\n",
    "hmr2['LSTM 100'] = hmr2['LSTM 100'].astype(float)\n",
    "hmr2['GRU 32'] = hmr2['GRU 32'].astype(float)\n",
    "hmr2['GRU 64'] = hmr2['GRU 64'].astype(float)\n",
    "hmr2['GRU 100'] = hmr2['GRU 100'].astype(float)\n",
    "hmr2['Naive '] = hmr2['Naive '].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be81764",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(hm.drop(['Naive '], axis=1, inplace=False), annot=True, cmap=sns.color_palette(\"Reds\",200), fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Heatmap de RMSE para cada modelo de red neuronal en cada estación de control\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6be173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(hmr2.drop(['Naive '], axis=1, inplace=False), annot=True, cmap=sns.color_palette(\"Blues_r\",200), fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Heatmap de R2 para cada modelo de red neuronal en cada estación de control\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96284fd",
   "metadata": {},
   "source": [
    "#### 4.6. Predicciones <a class=\"anchor\" id=\"4.6.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1314c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "predicciones = p[(p['MODELO'] == 'LSTM 100') & (p['ESTACION'] == '57')]['PREDICCIONES'].values[0][:168*1]\n",
    "\n",
    "valores_reales = df_e57['NivelesPM10'].iloc[len(idx_train_e57) + len(idx_val_e57):][:168*1]\n",
    "plt.plot(valores_reales, color='black', label='Valores reales')\n",
    "\n",
    "vals = df_e57['NivelesPM10'].iloc[len(idx_train_e57) + len(idx_val_e57):][:168*1]\n",
    "for i in range(len(predicciones)):\n",
    "    min_len = min(len(vals.index[i:i + len(predicciones[i])]), len(predicciones[i]))\n",
    "\n",
    "    plt.plot(\n",
    "        vals.index[i:i + min_len], \n",
    "        predicciones[i][:min_len], \n",
    "        label=f'Predicción desde {vals.index[i]} a {vals.index[i + min_len - 1]}' if i < 6 else \"\"\n",
    "    )\n",
    "\n",
    "plt.title('Predicciones para la estación 57 con LSTM 100')\n",
    "plt.xlabel('Horas')\n",
    "plt.ylabel('Valor de la Predicción')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "predicciones = p[(p['MODELO'] == 'LSTM 100') & (p['ESTACION'] == '55')]['PREDICCIONES'].values[0][:168*1]\n",
    "\n",
    "valores_reales = df_e55['NivelesPM10'].iloc[len(idx_train_e55) + len(idx_val_e55):][:168*1]\n",
    "plt.plot(valores_reales, color='black', label='Valores reales')\n",
    "\n",
    "vals = df_e55['NivelesPM10'].iloc[len(idx_train_e55) + len(idx_val_e55):][:168*1]\n",
    "for i in range(len(predicciones)):\n",
    "    min_len = min(len(vals.index[i:i + len(predicciones[i])]), len(predicciones[i]))\n",
    "\n",
    "    plt.plot(\n",
    "        vals.index[i:i + min_len], \n",
    "        predicciones[i][:min_len], \n",
    "        label=f'Predicción desde {vals.index[i]} a {vals.index[i + min_len - 1]}' if i < 6 else \"\"\n",
    "    )\n",
    "\n",
    "plt.title('Predicciones para la estación 55 con LSTM 100')\n",
    "plt.xlabel('Horas')\n",
    "plt.ylabel('Valor de la Predicción')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722c32f",
   "metadata": {},
   "source": [
    "#### 4.7. Veces que los modelos se superan unos a otros <a class=\"anchor\" id=\"4.6.\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = p['MODELO'].unique()\n",
    "tabla = pd.DataFrame(index=modelos, columns=modelos)\n",
    "\n",
    "for modelo1 in modelos:\n",
    "    for modelo2 in modelos:\n",
    "        if modelo1 != modelo2:\n",
    "            suma_dif = df_DMS_uni[(df_DMS_uni['MODELO 1'] == modelo1) & (df_DMS_uni['MODELO 2'] == modelo2)]['DIFERENCIA'].sum()\n",
    "            tabla.loc[modelo1, modelo2] = suma_dif\n",
    "\n",
    "for i in range(len(modelos)):\n",
    "    tabla.iloc[i, i] = '-'\n",
    "\n",
    "print()\n",
    "print(tabla)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1f275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
